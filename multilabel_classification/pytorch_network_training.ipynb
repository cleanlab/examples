{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural network for multi-label classification on the CelebA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train a Pytorch neural network for image tagging and use the model to produce out-of-sample predicted class probabilities for each image in the dataset. These are required inputs to find label errors in multi-label classification datasets with cleanlab. Here we consider a subset of the [CelebA](https://www.kaggle.com/datasets/jessicali9530/celeba-dataset) dataset, where each image may be tagged with one or more of the following tags: `['Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'Eyeglasses', 'No_Beard', 'Smiling']`, depending on which ones apply to the person depicted.\n",
    "\n",
    "This notebook only shows how to train the network and use it to produce `pred_probs`, using them to find label issues is demonstrated in cleanlab's Tutorial on [Multi-Label Classification](https://docs.cleanlab.ai/). Here we fit a state-of-the-art neural network initialized from a pretrained [TIMM](https://timm.fast.ai/) network backbone. You can use this same code to obtain a multi-label classifier (i.e. image tagging model) for *any* image dataset.\n",
    "\n",
    "We first need to download the dataset.\n",
    "If the google drive download script fails, please download the following links from [CelebA dataset](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) and paste them in the current directory\n",
    " * [Images](https://drive.google.com/uc?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM)\n",
    " * [Labels](https://drive.google.com/uc?id=0B7EVK8r0v71pblRyaVFSWGxPY0U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "def download_drive(url,output):\n",
    "    filename = gdown.download(url, output, quiet=False)\n",
    "    if filename is None:\n",
    "        print(f\"Downloading {url} failed, please download it from browser and paste it in {os.getcwd()}\")\n",
    "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pblRyaVFSWGxPY0U'\n",
    "output = 'list_attr_celeba.txt'\n",
    "if not os.path.exists(output):\n",
    "    download_drive(url,output)\n",
    "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM' # Usually errors out, Download them manually from the link below\n",
    "output = 'img_align_celeba.zip'\n",
    "if not os.path.exists(output):\n",
    "    download_drive(url,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq img_align_celeba.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import required dependencies (make sure you have installed these packages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as data\n",
    "from timm.optim import create_optimizer\n",
    "from timm.models import create_model\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.loader import create_loader\n",
    "from timm.utils import CheckpointSaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the dataset, and preprocess it to only keep the classes of interest (i.e. *image tags*) listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA =(\"image_id \" + open(\"list_attr_celeba.txt\").read()[7:]).splitlines()\n",
    "\n",
    "from collections import defaultdict\n",
    "q = DATA[0]\n",
    "d_data = defaultdict(list)\n",
    "ls = q.split()\n",
    "for i in q.split():\n",
    "    d_data[i] = []\n",
    "for j in DATA[1:]:\n",
    "    labels = j.split()\n",
    "    for k in range(0,len(labels)):\n",
    "        if k==0:\n",
    "            d_data[ls[k]].append(labels[k])\n",
    "        else:\n",
    "            # map -1 entries -> 0\n",
    "            ps = int((int(labels[k])+1)/2)\n",
    "            d_data[ls[k]].append(ps)\n",
    "            \n",
    "dat = pd.DataFrame.from_dict(d_data)\n",
    "\n",
    "\n",
    "selected = ['image_id',\n",
    "'Eyeglasses',\n",
    " 'Wearing_Earrings',\n",
    " 'Wearing_Hat',\n",
    " 'Wearing_Necklace',\n",
    " 'Wearing_Necktie',\n",
    " 'No_Beard',\n",
    " 'Smiling']\n",
    "\n",
    "def is_label(row):\n",
    "    for s in selected[1:]:\n",
    "        if row[s]!=0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_loc(i):\n",
    "    return os.path.join(os.getcwd(),'img_align_celeba/')+i\n",
    "\n",
    "dat_label = dat.apply(is_label,axis=1)\n",
    "dat_selected = dat[dat_label][selected]\n",
    "dat_selected['image_path'] = dat_selected['image_id'].map(lambda x:get_loc(x))\n",
    "selected[0] = 'image_path'\n",
    "\n",
    "df = dat_selected[selected]\n",
    "set_lab = {}\n",
    "for i,row in df.iterrows():\n",
    "    q = str(row.tolist()[1:])\n",
    "    if q not in set_lab:\n",
    "        set_lab[(str(q))]=len(set_lab)\n",
    "\n",
    "# Here we drop a couple rare class-combinations just to simplify stratified data splitting\n",
    "def get_lab(row):\n",
    "    q = str(row.tolist()[1:])\n",
    "    return set_lab[q]\n",
    "\n",
    "df['unique_label'] = df.apply(get_lab,axis=1)\n",
    "cnt = Counter(df['unique_label'])\n",
    "\n",
    "def drop(val):\n",
    "    if cnt[val]>10:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "is_drop = df['unique_label'].apply(lambda x:drop(x))\n",
    "df = df[is_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame contains the ID of each image and the classes (i.e. *tags*) that apply to this image.\n",
    "\n",
    "We define a general class of Pytorch neural networks for multi-label image classification. You can use this class for training models on *any* image dataset, and this class can utilize *any* pretrained [TIMM](https://timm.fast.ai/) network backbone. The `MultiLabelModel` below adds an appropriate output layer on top of the network backbone and then fine-tunes the entire network jointly on your multi-label classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelModel(nn.Module):\n",
    "    \"\"\" \n",
    "    Pytorch network for multi-label classification that can utilize any TIMM network backbone.\n",
    "    Some of this code is inspired by: https://github.com/yang-ruixin/PyTorch-Image-Models-Multi-Label-Classification\n",
    "    Note this network uses Sigmoid output activations because the predicted probabilities do not need to sum to 1\n",
    "    for multi-label classification, in which each image may belong to multiple classes rather than only one.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, n_classes, class_weights=None, verbose = False):\n",
    "        super().__init__()\n",
    "        self.base_model = model  # network backbone can be any TIMM model\n",
    "        self.num_classes = n_classes\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_loss(self, loss_fn, output, target):\n",
    "\n",
    "        return loss_fn(output, target)\n",
    "\n",
    "    def validate(self, loader):\n",
    "        self.eval();\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            m = nn.Sigmoid()\n",
    "            labels = []\n",
    "            preds = []\n",
    "            for batch_idx, (input, target) in enumerate(loader):\n",
    "                input = input.cuda()\n",
    "                labels.append(target.detach().cpu())\n",
    "                target = target.float().cuda()\n",
    "                output = m(self(input))\n",
    "                loss = self.get_loss(loss_fn, output, target)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pred_model = (output > 0.5).detach().cpu()\n",
    "                preds.append(pred_model)\n",
    "\n",
    "            num_of_batches_per_epoch = len(loader)\n",
    "            avg_loss = total_loss / num_of_batches_per_epoch\n",
    "            print(\"VALIDATION DATA STATS\")\n",
    "\n",
    "            print(\"AVERAGE LOSS:\", avg_loss)\n",
    "            preds = torch.cat(preds).int()\n",
    "            labels = torch.cat(labels).int()\n",
    "            acc_score = accuracy_score(labels, preds)\n",
    "            print(\"MULTILABEL accuracy score:\", acc_score)\n",
    "            per_class = []\n",
    "            for i in range(len(preds.T)):\n",
    "                per_class.append(accuracy_score(labels.T[i], preds.T[i]))\n",
    "            print(dataset_train.label_names)\n",
    "            print(per_class)\n",
    "            print('\\n\\n')\n",
    "        return avg_loss\n",
    "\n",
    "    def predict_proba(self, loader):\n",
    "        self.eval();\n",
    "        with torch.no_grad():\n",
    "            m = nn.Sigmoid()\n",
    "            preds = []\n",
    "            for batch_idx, (input, target) in enumerate(loader):\n",
    "                input = input.cuda()\n",
    "                output = m(self(input))\n",
    "                pred_model = output.detach().cpu()\n",
    "                preds.append(pred_model)\n",
    "            preds = torch.cat(preds)\n",
    "        return preds\n",
    "\n",
    "    def train_one_epoch(\n",
    "        self,\n",
    "        loader,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "    ):\n",
    "        sta = time.time()\n",
    "        second_order = hasattr(optimizer, \"is_second_order\") and optimizer.is_second_order\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        m = nn.Sigmoid()\n",
    "        labels = []\n",
    "        preds = []\n",
    "        ct = 0\n",
    "        for batch_idx, (input, target) in enumerate(loader):\n",
    "            input = input.cuda()\n",
    "            ct += 1\n",
    "            labels.append(target.detach().cpu())\n",
    "            target = target.float().cuda()\n",
    "            output = m(self(input))\n",
    "            loss = self.get_loss(loss_fn, output, target)\n",
    "            total_loss += loss.item()\n",
    "            pred_model = (output > 0.5).detach().cpu()\n",
    "            preds.append(pred_model)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(create_graph=second_order)\n",
    "            optimizer.step()\n",
    "            if ct % 80 == 0 and self.verbose:\n",
    "                print(\"LOSS:\", loss.item())\n",
    "        num_of_batches_per_epoch = len(loader)\n",
    "        avg_loss = total_loss / num_of_batches_per_epoch\n",
    "        print(\"TRAINING DATA STATS\")\n",
    "        print(\"AVERAGE LOSS:\", avg_loss)\n",
    "        preds = torch.cat(preds).int()\n",
    "        labels = torch.cat(labels).int()\n",
    "        acc_score = accuracy_score(labels, preds)\n",
    "        print(\"MULTILABEL accuracy score:\", acc_score)\n",
    "        per_class = []\n",
    "        for i in range(len(preds.T)):\n",
    "            per_class.append(accuracy_score(labels.T[i], preds.T[i]))\n",
    "        print(dataset_train.label_names)\n",
    "        print(per_class)\n",
    "        print('\\n\\n')\n",
    "        sto = time.time()\n",
    "        print(\"training time\", sto - sta)\n",
    "        return avg_loss\n",
    "    \n",
    "\n",
    "    def fit(self, loader_train, load_val, num_epochs=10):\n",
    "        if os.path.exists(\"weights_model\"):\n",
    "            print(\"removing weights directory\")\n",
    "            os.system('rm -rf weights_model')\n",
    "        os.mkdir(\"weights_model\")\n",
    "        args = SimpleNamespace()\n",
    "        args.weight_decay = 0\n",
    "        args.lr = 1e-4\n",
    "        args.opt = 'adam'\n",
    "        args.momentum = 0.9\n",
    "        args.sched = \"step\"\n",
    "\n",
    "        optimizer = create_optimizer(args, self)\n",
    "        saver = CheckpointSaver(\n",
    "            model=self,\n",
    "            optimizer=optimizer,\n",
    "            checkpoint_dir=\"weights_model\"\n",
    "        )\n",
    "        errs = []\n",
    "        num_of_data_train = len(loader_train.dataset.data)\n",
    "        for epoch in range(0, num_epochs):\n",
    "            loss_train = self.train_one_epoch(\n",
    "                loader_train,\n",
    "                optimizer,\n",
    "                loss_fn,\n",
    "            )\n",
    "            loss_val = self.validate(loader_val)\n",
    "            errs.append([loss_train, loss_val])\n",
    "            saver.save_checkpoint(epoch, metric=loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a wrapper class for training our `MultiLabel` model on multi-label classification image datasets. You can easily apply this to your own datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMultiLabel(data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            annotation_path=None,\n",
    "            df = None,\n",
    "            transform=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.label_names = []\n",
    "        if annotation_path is None:\n",
    "            assert df is not None\n",
    "        else:\n",
    "            df = pd.read_csv(annotation_path)\n",
    "        \n",
    "        cols = df.columns\n",
    "        self.label_names = list(cols[1:-1])\n",
    "        for i,row in df.iterrows():\n",
    "            lb = []\n",
    "            for j in cols:\n",
    "                if j=='unique_label':\n",
    "                    continue\n",
    "                if j=='image_path':\n",
    "                    self.data.append(row[j])\n",
    "                else:\n",
    "                    lb.append(float(row[j]))\n",
    "            self.labels.append(lb)\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        return img, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "  Re-initializes model weights, eg. between cross-validation folds. \n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "def create_df(pred_probs, dataset):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with image_path and predicted probabilities for each image.\n",
    "    \"\"\"\n",
    "    ls = dataset_val.label_names\n",
    "    cl = defaultdict(list)\n",
    "    cl['image_path'] = dataset.data\n",
    "    for i in range(0,len(ls)):\n",
    "        cl[ls[i]] = pred_val.T[i].tolist()\n",
    "    return pd.DataFrame.from_dict(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `DatasetMultiLabel` and `MultiLabelModel` for the Celeb-A dataset. Here we use the [efficientnet_b0](https://rwightman.github.io/pytorch-image-models/models/tf-efficientnet/) backbone for our neural network, but you can easily use any other TIMM backbone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetMultiLabel(df = df)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "model = create_model(\n",
    "    'efficientnet_b0',  # you can replace this with any TIMM backbone\n",
    "    num_classes=len(dataset.labels[0]),\n",
    ")\n",
    "data_config = resolve_data_config(\n",
    "       args = {}, model=model\n",
    "    )\n",
    "\n",
    "model = MultiLabelModel(\n",
    "        model,\n",
    "        n_classes=len(dataset.labels[0]),\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train this network using K-fold cross validation. This allows us to obtain **out-of-sample** predictions for each image in the dataset (i.e. predictions from a copy of the model which never saw this image during training). Out-of-sample predictions are less prone to overfitting, and thus better suited for finding label issues. From each fold of cross-validation, we store the predicted class probabilities for the images that were out-of-sample in a DataFrame `df_pred`. These predictions are subsequently used for finding label issues in cleanlab's Tutorial on [Multi-Label Classification](https://docs.cleanlab.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 4  # number of cross-validation splits (higher values will take longer but give better results)\n",
    "skf = StratifiedKFold(n_splits=num_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing weights directory\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.4262694525561977\n",
      "MULTILABEL accuracy score: 0.2686423390752493\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9217050090661831, 0.7923702402538532, 0.9437259179510427, 0.8594670784224842, 0.9207417271078876, 0.891602447869447, 0.5495594401631912]\n",
      "\n",
      "\n",
      "\n",
      "training time 358.5886492729187\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.3767583860165399\n",
      "MULTILABEL accuracy score: 0.295736047079819\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9346066413138159, 0.7948118719326945, 0.9508168858484353, 0.8679173128810894, 0.921880643310884, 0.8969810278527268, 0.5939153158129554]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.3613069791917572\n",
      "MULTILABEL accuracy score: 0.29702374206708976\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9345818223028105, 0.7951325929283771, 0.948931890299184, 0.8666775271985494, 0.9214287737987308, 0.8968509179510427, 0.5888131799637353]\n",
      "\n",
      "\n",
      "\n",
      "training time 351.6937503814697\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.3243455143402452\n",
      "MULTILABEL accuracy score: 0.33100342051031467\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9614820795003081, 0.8063481272174892, 0.9578491151288534, 0.8679598036924515, 0.9220293611506511, 0.8975971446174764, 0.6427372580679428]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.3190206883784218\n",
      "MULTILABEL accuracy score: 0.35297909111514053\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9535499773345422, 0.8023713735267453, 0.9550161491387126, 0.8668262692656392, 0.9229728581142339, 0.898819979601088, 0.660917667724388]\n",
      "\n",
      "\n",
      "\n",
      "training time 350.5784282684326\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.24707807470922885\n",
      "MULTILABEL accuracy score: 0.47772419214344897\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9777135694406085, 0.8411693471286834, 0.9696403152818203, 0.865559072850496, 0.9244725828039686, 0.9073275404193843, 0.818033100342051]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.27576278281125394\n",
      "MULTILABEL accuracy score: 0.42304368766999095\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9660230621033545, 0.8188959088848595, 0.9619999433363554, 0.867116670444243, 0.9249135879419764, 0.9073761899365367, 0.7510270285584769]\n",
      "\n",
      "\n",
      "\n",
      "training time 352.9354214668274\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.2616351339156213\n",
      "MULTILABEL accuracy score: 0.5321974123095881\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9806454354245895, 0.8598228133166202, 0.9736769423612144, 0.8668125517856764, 0.9263634239095796, 0.9242388833414774, 0.8650491831141516]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.24971055313742171\n",
      "MULTILABEL accuracy score: 0.4626374093381686\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9734459995466909, 0.8329980734360835, 0.9671846668177697, 0.8674920670897552, 0.927732604261106, 0.917873130099728, 0.7874617520398912]\n",
      "\n",
      "\n",
      "\n",
      "training time 353.0294861793518\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.19497127221573307\n",
      "MULTILABEL accuracy score: 0.5535065542076526\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.988081327412947, 0.8725913021309142, 0.9807729078586755, 0.8680447853151756, 0.9288278909685781, 0.9389407040727442, 0.8681085215322186]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.23327327786471125\n",
      "MULTILABEL accuracy score: 0.4883839528558477\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9776249433363554, 0.8416888599274706, 0.970584485494107, 0.8674637352674524, 0.9292696056210336, 0.9257422937443336, 0.8104034451495921]\n",
      "\n",
      "\n",
      "\n",
      "training time 353.1277017593384\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.1814094968241356\n",
      "MULTILABEL accuracy score: 0.580636937262317\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9903970766321782, 0.8742059529626718, 0.9817289511143216, 0.8676411226072361, 0.9285729461004057, 0.9470776944485755, 0.8984682062503984]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.22061361708186047\n",
      "MULTILABEL accuracy score: 0.5088961922030825\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9806351994560291, 0.8482618427017226, 0.9731910131459656, 0.8676549750679964, 0.9328889959202176, 0.9311678377153219, 0.825100577969175]\n",
      "\n",
      "\n",
      "\n",
      "training time 353.07073426246643\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.16863207319629905\n",
      "MULTILABEL accuracy score: 0.5999915018377275\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9925003717945994, 0.8844037476895621, 0.9844058722301302, 0.8680235399094945, 0.9350952856444794, 0.9537275064267352, 0.9046931101149377]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.2125667459715634\n",
      "MULTILABEL accuracy score: 0.5224317203082502\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9815772325475974, 0.8536165571169537, 0.9737293177697189, 0.8677399705349048, 0.934142679057117, 0.9359559156844969, 0.833982604261106]\n",
      "\n",
      "\n",
      "\n",
      "training time 353.6042172908783\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.16432252999030703\n",
      "MULTILABEL accuracy score: 0.6103592598100661\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9933926788332024, 0.8905649153370584, 0.9854893879198623, 0.8680660307208566, 0.9374110348637107, 0.9543436231914848, 0.908028638806858]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.20705392777730633\n",
      "MULTILABEL accuracy score: 0.5301167271078876\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9833054737080689, 0.8558760199456029, 0.9759816976427924, 0.8678745466908432, 0.93492888712602, 0.9379674750679964, 0.8406759972801451]\n",
      "\n",
      "\n",
      "\n",
      "training time 352.49580812454224\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.16146331879755724\n",
      "MULTILABEL accuracy score: 0.6115490025282033\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.994008795597952, 0.893241836452867, 0.9861904863073361, 0.8683209755890289, 0.9393443667806837, 0.9522828188404258, 0.9062227793239712]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.20218737006646767\n",
      "MULTILABEL accuracy score: 0.5393458182230281\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9842404238440616, 0.8585037964641886, 0.9763925090661831, 0.8678745466908432, 0.9352972008159565, 0.9404748413417952, 0.8481839301903898]\n",
      "\n",
      "\n",
      "\n",
      "training time 353.427463054657\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.15529315695976434\n",
      "MULTILABEL accuracy score: 0.6257834243344876\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9941150226263571, 0.89413414349147, 0.987210265780025, 0.8682784847776668, 0.9413839257260618, 0.9598886740742314, 0.914869659436147]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.19757173848038678\n",
      "MULTILABEL accuracy score: 0.5468183363553943\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9848141432456936, 0.8608695036264733, 0.9775257819582955, 0.8681507819582955, 0.9370396078875793, 0.9421605847688124, 0.854516092475068]\n",
      "\n",
      "\n",
      "\n",
      "training time 352.30041551589966\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.1545726609436552\n",
      "MULTILABEL accuracy score: 0.6275255476003314\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9942212496547621, 0.8947927510675816, 0.9867428668550426, 0.868448448023115, 0.9420425333021734, 0.9610359259810066, 0.9150608680872762]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.1943148595440528\n",
      "MULTILABEL accuracy score: 0.5518401518585675\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9855011899365367, 0.8630510539437897, 0.9784040684496826, 0.8681932796917498, 0.9367987873980055, 0.9441154805077062, 0.8568322189483227]\n",
      "\n",
      "\n",
      "\n",
      "training time 353.6653382778168\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.15092285945201697\n",
      "MULTILABEL accuracy score: 0.6337079606535087\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9946886485797446, 0.8978520894856488, 0.9878051371390937, 0.8682572393719858, 0.9430198219635004, 0.9622681595105058, 0.9182689243451103]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.19107352807807143\n",
      "MULTILABEL accuracy score: 0.558639789211242\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9863369786944697, 0.8647155485040798, 0.9785528105167725, 0.8689228241160472, 0.9377054057116954, 0.9458437216681777, 0.8620381912964642]\n",
      "\n",
      "\n",
      "\n",
      "training time 353.121693611145\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.1521785281618814\n",
      "MULTILABEL accuracy score: 0.6296925789797956\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9949223480422359, 0.8979795619197348, 0.9877201555163696, 0.8683847118060719, 0.9428286133123712, 0.9563619367311819, 0.9181839427223862]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.1882758161605269\n",
      "MULTILABEL accuracy score: 0.5624362533998187\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9871586015412511, 0.8665996146872167, 0.9789707048957389, 0.868427017225748, 0.9383287058023572, 0.9464032751586582, 0.8650767792384406]\n",
      "\n",
      "\n",
      "\n",
      "training time 351.7733905315399\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.1903194089865555\n",
      "MULTILABEL accuracy score: 0.6334105249739744\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9929465253139008, 0.8972147273152181, 0.98710403875162, 0.8683847118060719, 0.9440183560305084, 0.963457902228643, 0.9177165437974039]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.1862856383728419\n",
      "MULTILABEL accuracy score: 0.565000283318223\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9877535698096102, 0.8672158318223028, 0.9796648345421578, 0.8688944922937444, 0.9376345761559384, 0.9472461468721668, 0.8662171350861287]\n",
      "\n",
      "\n",
      "\n",
      "training time 352.53416633605957\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.14817228584307368\n",
      "MULTILABEL accuracy score: 0.6411013618305041\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9947948756081497, 0.9002103295162421, 0.9882725360640762, 0.8691707918162698, 0.9457604792963522, 0.9649875714376766, 0.9186513416473687]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.18409081540008837\n",
      "MULTILABEL accuracy score: 0.5709145512239348\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9873073436083409, 0.8695744560290117, 0.9805289551223935, 0.8690644832275612, 0.9395682230281052, 0.948004023118767, 0.8684057683590208]\n",
      "\n",
      "\n",
      "\n",
      "training time 353.26200914382935\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.14677373018196743\n",
      "MULTILABEL accuracy score: 0.6416112515668486\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9954109923728993, 0.8999128938367078, 0.9879751003845418, 0.8681297669378997, 0.9438059019736982, 0.9646688903524613, 0.9215194714143067]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.18235593517787976\n",
      "MULTILABEL accuracy score: 0.5734502493200363\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9878031504986401, 0.8708635539437897, 0.980514789211242, 0.86890157524932, 0.9398302923844062, 0.9489531391659112, 0.8706510652765186]\n",
      "\n",
      "\n",
      "\n",
      "training time 354.0899291038513\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.14414783958953037\n",
      "MULTILABEL accuracy score: 0.6478149100257069\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9957084280524336, 0.9025048333297925, 0.9872952474027492, 0.8687033928912873, 0.946631540929274, 0.9661560687501328, 0.9244513373982877]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.1805467230123438\n",
      "MULTILABEL accuracy score: 0.5755326382592928\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9880368880326382, 0.8710618766999093, 0.9808901858567544, 0.8693832162284678, 0.9399507026291931, 0.9496543517679057, 0.8716922597461468]\n",
      "\n",
      "\n",
      "\n",
      "training time 352.2673830986023\n",
      "VALIDATION DATA STATS\n",
      "AVERAGE LOSS: 0.14492283086292446\n",
      "MULTILABEL accuracy score: 0.6477299284029828\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9955809556183476, 0.9018887165650428, 0.9879326095731799, 0.8689795831651406, 0.9460154241645244, 0.9667084492978394, 0.9247062822664599]\n",
      "\n",
      "\n",
      "\n",
      "TRAINING DATA STATS\n",
      "AVERAGE LOSS: 0.17945134213172298\n",
      "MULTILABEL accuracy score: 0.5793999320036265\n",
      "['Eyeglasses', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Necklace', 'Wearing_Necktie', 'No_Beard', 'Smiling']\n",
      "[0.9883910358114234, 0.8732575929283771, 0.9812726654578422, 0.8695886219401632, 0.9400852787851315, 0.9504405598368088, 0.8734842475067996]\n",
      "\n",
      "\n",
      "\n",
      "training time 350.9865779876709\n"
     ]
    }
   ],
   "source": [
    "ct = 1\n",
    "for train_index, test_index in skf.split(df,df['unique_label']):\n",
    "    if ct!=1:\n",
    "        model.apply(reset_weights);\n",
    "    dataset_train = DatasetMultiLabel(df = df.iloc[train_index])\n",
    "    dataset_val = DatasetMultiLabel(df = df.iloc[test_index])\n",
    "    loader_train = create_loader(\n",
    "        dataset_train,\n",
    "        input_size=data_config[\"input_size\"],\n",
    "        batch_size=64,\n",
    "        is_training=True,\n",
    "        mean=data_config[\"mean\"],\n",
    "        std=data_config[\"std\"],\n",
    "       interpolation=data_config[\"interpolation\"],\n",
    "    )\n",
    "    loader_val = create_loader(\n",
    "        dataset_val,\n",
    "        input_size=data_config[\"input_size\"],\n",
    "        batch_size=64,\n",
    "        is_training=False,\n",
    "        mean=data_config[\"mean\"],\n",
    "        std=data_config[\"std\"],\n",
    "        interpolation=data_config[\"interpolation\"],\n",
    "\n",
    "    )\n",
    "    model.fit(loader_train,loader_val,num_epochs=40)\n",
    "    checkpoint = torch.load(\"weights_model/model_best.pth.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    pred_val = model.predict_proba(loader_val)\n",
    "    df_pred = create_df(pred_val,dataset_val)\n",
    "    df_pred.to_csv(str(ct)+\"_fold.csv\",index=False)\n",
    "    ct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>Eyeglasses</th>\n",
       "      <th>Wearing_Earrings</th>\n",
       "      <th>Wearing_Hat</th>\n",
       "      <th>Wearing_Necklace</th>\n",
       "      <th>Wearing_Necktie</th>\n",
       "      <th>No_Beard</th>\n",
       "      <th>Smiling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/aditya/chipbrain/img_align_celeba/img_align_c...</td>\n",
       "      <td>0.076085</td>\n",
       "      <td>0.762237</td>\n",
       "      <td>0.202235</td>\n",
       "      <td>0.209842</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>0.642169</td>\n",
       "      <td>0.531744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/aditya/chipbrain/img_align_celeba/img_align_c...</td>\n",
       "      <td>0.071449</td>\n",
       "      <td>0.249391</td>\n",
       "      <td>0.013061</td>\n",
       "      <td>0.185279</td>\n",
       "      <td>0.109860</td>\n",
       "      <td>0.903723</td>\n",
       "      <td>0.577488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/aditya/chipbrain/img_align_celeba/img_align_c...</td>\n",
       "      <td>0.040210</td>\n",
       "      <td>0.191505</td>\n",
       "      <td>0.011554</td>\n",
       "      <td>0.147552</td>\n",
       "      <td>0.098694</td>\n",
       "      <td>0.949420</td>\n",
       "      <td>0.681327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/aditya/chipbrain/img_align_celeba/img_align_c...</td>\n",
       "      <td>0.395407</td>\n",
       "      <td>0.142709</td>\n",
       "      <td>0.398333</td>\n",
       "      <td>0.175002</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>0.816407</td>\n",
       "      <td>0.178379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/aditya/chipbrain/img_align_celeba/img_align_c...</td>\n",
       "      <td>0.038674</td>\n",
       "      <td>0.188319</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>0.196935</td>\n",
       "      <td>0.029655</td>\n",
       "      <td>0.961010</td>\n",
       "      <td>0.599208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47064</th>\n",
       "      <td>/aditya/chipbrain/img_align_celeba/img_align_c...</td>\n",
       "      <td>0.066824</td>\n",
       "      <td>0.181551</td>\n",
       "      <td>0.019896</td>\n",
       "      <td>0.189869</td>\n",
       "      <td>0.072942</td>\n",
       "      <td>0.894503</td>\n",
       "      <td>0.498349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47065</th>\n",
       "      <td>/aditya/chipbrain/img_align_celeba/img_align_c...</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.135114</td>\n",
       "      <td>0.091082</td>\n",
       "      <td>0.112658</td>\n",
       "      <td>0.067527</td>\n",
       "      <td>0.840356</td>\n",
       "      <td>0.409490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47066</th>\n",
       "      <td>/aditya/chipbrain/img_align_celeba/img_align_c...</td>\n",
       "      <td>0.485218</td>\n",
       "      <td>0.092114</td>\n",
       "      <td>0.306023</td>\n",
       "      <td>0.082606</td>\n",
       "      <td>0.067753</td>\n",
       "      <td>0.795474</td>\n",
       "      <td>0.222876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47067</th>\n",
       "      <td>/aditya/chipbrain/img_align_celeba/img_align_c...</td>\n",
       "      <td>0.034920</td>\n",
       "      <td>0.177051</td>\n",
       "      <td>0.040939</td>\n",
       "      <td>0.152979</td>\n",
       "      <td>0.051364</td>\n",
       "      <td>0.925292</td>\n",
       "      <td>0.478864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47068</th>\n",
       "      <td>/aditya/chipbrain/img_align_celeba/img_align_c...</td>\n",
       "      <td>0.137694</td>\n",
       "      <td>0.190992</td>\n",
       "      <td>0.038843</td>\n",
       "      <td>0.101974</td>\n",
       "      <td>0.124286</td>\n",
       "      <td>0.833890</td>\n",
       "      <td>0.448720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47069 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image_path  Eyeglasses  \\\n",
       "0      /aditya/chipbrain/img_align_celeba/img_align_c...    0.076085   \n",
       "1      /aditya/chipbrain/img_align_celeba/img_align_c...    0.071449   \n",
       "2      /aditya/chipbrain/img_align_celeba/img_align_c...    0.040210   \n",
       "3      /aditya/chipbrain/img_align_celeba/img_align_c...    0.395407   \n",
       "4      /aditya/chipbrain/img_align_celeba/img_align_c...    0.038674   \n",
       "...                                                  ...         ...   \n",
       "47064  /aditya/chipbrain/img_align_celeba/img_align_c...    0.066824   \n",
       "47065  /aditya/chipbrain/img_align_celeba/img_align_c...    0.047900   \n",
       "47066  /aditya/chipbrain/img_align_celeba/img_align_c...    0.485218   \n",
       "47067  /aditya/chipbrain/img_align_celeba/img_align_c...    0.034920   \n",
       "47068  /aditya/chipbrain/img_align_celeba/img_align_c...    0.137694   \n",
       "\n",
       "       Wearing_Earrings  Wearing_Hat  Wearing_Necklace  Wearing_Necktie  \\\n",
       "0              0.762237     0.202235          0.209842         0.003341   \n",
       "1              0.249391     0.013061          0.185279         0.109860   \n",
       "2              0.191505     0.011554          0.147552         0.098694   \n",
       "3              0.142709     0.398333          0.175002         0.017094   \n",
       "4              0.188319     0.006439          0.196935         0.029655   \n",
       "...                 ...          ...               ...              ...   \n",
       "47064          0.181551     0.019896          0.189869         0.072942   \n",
       "47065          0.135114     0.091082          0.112658         0.067527   \n",
       "47066          0.092114     0.306023          0.082606         0.067753   \n",
       "47067          0.177051     0.040939          0.152979         0.051364   \n",
       "47068          0.190992     0.038843          0.101974         0.124286   \n",
       "\n",
       "       No_Beard   Smiling  \n",
       "0      0.642169  0.531744  \n",
       "1      0.903723  0.577488  \n",
       "2      0.949420  0.681327  \n",
       "3      0.816407  0.178379  \n",
       "4      0.961010  0.599208  \n",
       "...         ...       ...  \n",
       "47064  0.894503  0.498349  \n",
       "47065  0.840356  0.409490  \n",
       "47066  0.795474  0.222876  \n",
       "47067  0.925292  0.478864  \n",
       "47068  0.833890  0.448720  \n",
       "\n",
       "[47069 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'image_loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3799\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3800\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3801\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_loc'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d807977dade4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_loc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_loc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pred_probs.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3804\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3807\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3800\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3804\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_loc'"
     ]
    }
   ],
   "source": [
    "dfl = []\n",
    "for i in range(1,num_splits+1):\n",
    "    dfl.append(pd.read_csv(str(i)+\"_fold.csv\"))\n",
    "\n",
    "cols = dfl[0].columns[1:]\n",
    "df_pred = pd.concat(dfl,axis=0)\n",
    "df_pred['image_path'] = (df_pred['image_loc'].map(lambda x:x.split('/')[-1]))\n",
    "df_pred = df_pred.drop(columns=['image_loc'])\n",
    "df_pred.set_index('image_path').to_csv(\"pred_probs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
