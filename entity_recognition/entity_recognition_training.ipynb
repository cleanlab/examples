{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d2e007",
   "metadata": {},
   "source": [
    "# Training Entity Recognition Model for Token Classification Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6676421",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train a NLP model for entity recognition and use it to produce out-of-sample predicted probabilities for each token. ",
    "These are required inputs to find label issues in token classification datasets with cleanlab. ",
    "The specific token classification task we consider here is Named Entity Recognition with the [CoNLL-2003 dataset](https://deepai.org/dataset/conll-2003-english), and we train a Transformer network from [HuggingFace's transformers library](https://github.com/huggingface/transformers). ",
    "This notebook demonstrates how to produce the `pred_probs`, using them to find label issues is demonstrated in cleanlab's [Token Classification Tutorial](https://docs.cleanlab.ai/). \n",
    "\n",
    "**Overview of what we'll do in this notebook:** \n",
    "- Read and process text datasets with per-token labels in the CoNLL format. \n",
    "- Compute out-of-sample predicted probabilities by training a Transformer network via cross-validation. \n",
    "- Aggregate subword-level tokens\\* into word-level tokens which are more individually meaningful. \n",
    "\n",
    "\\* In NLP, tokens typically refer to words or punctuation marks, but modern tokenizers used with Transformers often break down longer words into smaller subwords. To avoid confusion, we use \"word-level tokens\" to refer to the individual given tokens in the original dataset (with a separate class label provided for each such token). Tokens obtained from processing the raw text with tokenizers are referred as \"subword-level tokens\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da020bc",
   "metadata": {},
   "source": [
    "## 1. Fetch data and load required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://data.deepai.org/conll2003.zip && mkdir -p data \n",
    "!unzip conll2003.zip -d data/ && rm conll2003.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1349304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package versions we used: tqdm==4.64.0 transformers==4.22.0.dev0 numpy==1.23.0 sklearn==1.1.1 \n",
    "import os \n",
    "import warnings \n",
    "import string \n",
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline \n",
    "from sklearn.metrics import balanced_accuracy_score \n",
    "import nltk \n",
    "from cleanlab.internal.token_classification_utils import get_sentence, filter_sentence, process_token, mapping, merge_probs \n",
    "from bert import Ner \n",
    "from token_classification_tutorial_utils import create_folds, modified, get_pred_probs, to_dict \n",
    "from run_ner import train \n",
    "\n",
    "nltk.download(\"punkt\") \n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# Disable `TOKENIZERS_PARALLELISM` if multiple processors exist. \n",
    "if os.cpu_count() > 1: \n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e32604",
   "metadata": {},
   "source": [
    "## 2. Load the CONLL-2003 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80cd5a9",
   "metadata": {},
   "source": [
    "CONLL-2003 data are in the following format: \n",
    "\n",
    "`-DOCSTART- -X- -X- O` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of first sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "`[empty line]` \n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` <- Start of second sentence \n",
    "\n",
    "`...`\n",
    "\n",
    "`[word] [pos_tags] [chunk_tags] [ner_tags]` \n",
    "\n",
    "In our example, we focus on the `ner_tags` (named-entity recognition tags), which include the following classes: \n",
    "\n",
    "| `ner_tags` |             Description              |\n",
    "|:----------:|:------------------------------------:|\n",
    "|     `O`    |      Other (not a named entity)      |\n",
    "|   `B-MIS`  | Beginning of a miscellaneous entity  |\n",
    "|   `I-MIS`  |         Miscellaneous entity         |\n",
    "|   `B-PER`  |     Beginning of a person entity     |\n",
    "|   `I-PER`  |            Person entity             |\n",
    "|   `B-ORG`  | Beginning of an organization entity  |\n",
    "|   `I-ORG`  |         Organization entity          |\n",
    "|   `B-LOC`  |    Beginning of a location entity    |\n",
    "|   `I-LOC`  |           Location entity            | \n",
    "\n",
    "For more information, see [here](https://paperswithcode.com/dataset/conll-2003). We cast all-caps words into lowercase except for the first character (eg. `JAPAN` -> `Japan`), to discourage the tokenizer from breaking such words into multiple subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871730b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = ['data/train.txt', 'data/valid.txt', 'data/test.txt'] \n",
    "entities = ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "entity_map = {entity: i for i, entity in enumerate(entities)} \n",
    "\n",
    "# This code is adapted from: https://github.com/kamalkraj/BERT-NER/blob/dev/run_ner.py \n",
    "def readfile(filepath, sep=' '): \n",
    "    lines = open(filepath)\n",
    "    \n",
    "    data, sentence, label = [], [], []\n",
    "    for line in lines:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, label))\n",
    "                sentence, label = [], []\n",
    "            continue\n",
    "        splits = line.split(sep) \n",
    "        word = splits[0]\n",
    "        if len(word) > 0 and word[0].isalpha() and word.isupper():\n",
    "            word = word[0] + word[1:].lower()\n",
    "        sentence.append(word)\n",
    "        label.append(entity_map[splits[-1][:-1]])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, label))\n",
    "        \n",
    "    given_words = [d[0] for d in data] \n",
    "    given_labels = [d[1] for d in data] \n",
    "    \n",
    "    return given_words, given_labels \n",
    "\n",
    "given_words, given_labels = [], [] \n",
    "\n",
    "for filepath in filepaths: \n",
    "    words, labels = readfile(filepath) \n",
    "    given_words.extend(words) \n",
    "    given_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12148277",
   "metadata": {},
   "source": [
    "`given_words` and `given_labels` above are strings/labels corresponding to each word-level token, represented as nested lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d505f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0  # change this to view a different example from the dataset \n",
    "\n",
    "print('Word\\t\\tLabel\\tEntity\\n-------------------------------') \n",
    "for word, label in zip(given_words[i], given_labels[i]): \n",
    "    print('{:14s}{:3d}\\t{:10s}'.format(word, label, entities[label])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8639b",
   "metadata": {},
   "source": [
    "We next apply minor pre-processing for readability. Sentences containing the `#` character are removed for simplicity, because this special character is later used to represent subword-tokens by the sentence tokenizers used in HuggingFace (See section 4). We also remove single token sentences with `len(sentence) <= 1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(map(get_sentence, given_words)) \n",
    "\n",
    "sentences, mask = filter_sentence(sentences) \n",
    "given_words = [words for m, words in zip(mask, given_words) if m] \n",
    "given_labels = [labels for m, labels in zip(mask, given_labels) if m] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c496b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of sentences: %d' % len(sentences)) \n",
    "print(sentences[0])  # display first sentence in the processed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1b2b0",
   "metadata": {},
   "source": [
    "## 3. Train Token Classification Model using Cross-Validation \n",
    "\n",
    "To later find label issues in the training dataset, we first compute out-of-sample predicted probabilities (`pred_probs`) using cross-validation. We start by partitioning the dataset into `k = 5` disjoint folds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797df997",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [[]] \n",
    "for filepath in filepaths: \n",
    "    for line in open(filepath) : \n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "            if len(lines[-1]) > 0: \n",
    "                lines.append([]) \n",
    "        else: \n",
    "            lines[-1].append(line) \n",
    "        \n",
    "lines = lines[:-1] \n",
    "lines = [line for m, line in zip(mask, lines) if m] \n",
    "\n",
    "k = 5 \n",
    "indices = create_folds(lines, k=k) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304653aa",
   "metadata": {},
   "source": [
    "We train one model for each fold's training/testing pair: \n",
    "\n",
    "- Warning! The following code will take a long time to execute, and is recommended to run on GPU, otherwise it will take forever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c877f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k): \n",
    "    if os.path.exists('folds/fold%d/model/' % i): \n",
    "        print('Model %d already exists, skipping...' % i) \n",
    "    else: \n",
    "        print('Training model on fold %d (out of %d) of cross-validation...' % (i, k)) \n",
    "        train(data_dir='folds/fold%d' % i, \n",
    "              bert_model='bert-base-cased', \n",
    "              task_name='ner', \n",
    "              output_dir='folds/fold%d/model' % i, \n",
    "              max_seq_length=256, \n",
    "              do_train=True, \n",
    "              num_train_epochs=10, \n",
    "              warmup_proportion=0.1,\n",
    "              train_batch_size=16\n",
    "        ) \n",
    "        print('Model %d saved' % i) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80621481",
   "metadata": {},
   "source": [
    "## 4. Compute Out-of-Sample Predicted Probabilities \n",
    "\n",
    "We obtain predicted class probabilities for each token using the model in which this token was held out during training (i.e. its sentence was part of the validation fold above). ",
    "Note that most modern tokenizers break sentences into subword-level tokens (smaller units than word-level tokens). For example, the following sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0  # change this to view a different example from the dataset \n",
    "print(sentences[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d0119",
   "metadata": {},
   "source": [
    "is tokenized into: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08446655",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ner(\"folds/fold0/model/\") \n",
    "tokens = model.tokenize(sentences[i])[0] \n",
    "print(tokens) \n",
    "tokens = [token.replace('#', '') for token in tokens] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493203d3",
   "metadata": {},
   "source": [
    "`##` here is appended by the tokenizer to indicate that the token is a subword. Let's collect both the predicted class probabilities and strings for each token in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fac29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens, sentence_probs = {}, {} \n",
    "for i in range(k): \n",
    "    model = Ner(\"folds/fold%d/model/\" % i) \n",
    "    for index in tqdm(indices[i]): \n",
    "        sentence_probs[index], sentence_tokens[index] = model.predict(sentences[index]) \n",
    "        \n",
    "sentence_tokens = [sentence_tokens[i] for i in range(len(sentences))] \n",
    "sentence_probs = [sentence_probs[i] for i in range(len(sentences))] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a24f4",
   "metadata": {},
   "source": [
    "Most tokenizers partition sentences into subword-level tokens without altering the characters. However, you should verify whether any characters are modified, particularly for edge cases such as single or double quotations. In this example, the tokenizer has broken down double quotations `\"` into two `'`s or two `` ` ``s. The following code checks if any characters in the sentences were modified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ff9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modified(given_words, sentence_tokens)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b498649",
   "metadata": {},
   "source": [
    "Given that some characters were modified, we should map `sentence_tokens` back to the characters from the original dataset, so that we can better compare predicted labels with the given labels to spot label issues. This mapping differs between different models, and may not be required by many tokenizers. We should not work with modified tokens directly because we lack their given labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09fe4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to map `sentence_tokens` back to the characters from the original text \n",
    "replace = [('#', ''), ('``', '\"'), (\"''\", '\"')] \n",
    "sentence_tokens = [[process_token(token, replace) for token in sentence_tokens[i]] for i in range(len(sentences))] \n",
    "\n",
    "for i in range(len(sentences)): \n",
    "    short = ''.join(given_words[i]) \n",
    "    if \"''\" in short: \n",
    "        processed_tokens, processed_probs = [], [] \n",
    "        for token, prob in zip(sentence_tokens[i], sentence_probs[i]): \n",
    "            if token != '\"': \n",
    "                processed_tokens.append(token) \n",
    "                processed_probs.append(prob) \n",
    "            else: \n",
    "                for _ in range(2): \n",
    "                    processed_tokens.append(\"'\") \n",
    "                    processed_probs.append(prob) \n",
    "        sentence_tokens[i] = [token for token in processed_tokens] \n",
    "        sentence_probs[i] = np.array([prob for prob in processed_probs]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00edbe08",
   "metadata": {},
   "source": [
    "In this example, we are more interested in severe types of mislabels, such as `B-LOC` vs. `B-PER`, instead of `B-LOC` vs. `I-LOC`. Therefore, we discard the `B-` and `I-` prefixes, and get the model-predicted probabilities for each subword-level token. The merged entities are `[O, MIS, PER, ORG, LOC]`, which correspond to the classes in our token classification task. In cleanlab's Token Classification tutorial, we will use the probabilistic predictions from this trained model to identify instances where the class label was incorrectly chosen for particular tokens. As shown below:\n",
    "\n",
    "- `given_maps` is an array of length equal to the original number of entities, such that `given_maps[i]` is the mapped entity of the i'th entity \n",
    "- `model_maps` is an array of length equal to the number of model predicted labels, such that `model_maps[i]` is the mapped entity of the i'th model predicted entity. If `model_maps[i] < 0`, it indicates that the entity does not map to a valid named entity. This usually occurs when the model predicted entities include start/end tags. If `np.any(model_maps < 0)`, `pred_probs` will be normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_maps = [0, 1, 1, 2, 2, 3, 3, 4, 4] \n",
    "model_maps = [-1, 0, 1, 1, 2, 2, 3, 3, 4, 4, -1, -1] \n",
    "given_labels = [mapping(labels, maps=given_maps) for labels in given_labels] \n",
    "sentence_probs = [merge_probs(pred_prob, maps=model_maps) for pred_prob in sentence_probs] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb16f9cf",
   "metadata": {},
   "source": [
    "Specifically, `merge_probs` takes in two parameters: \n",
    "\n",
    "- `probs`: `np.array` of shape `(N, L)`, where `N` is the number of tokens in the sentence, and `L` is the number of classes of the model. \n",
    "- `maps`: `list` of length `L`, where `L` is the number of classes of the model, with details specified above in `model_maps`. \n",
    "\n",
    "and returns: \n",
    "\n",
    "- `probs_merged`: `np.array` of shape `(N, K)`, where `N` is the number of tokens in the sentence, and `K` is the number of classes of the new set of entities. \n",
    "\n",
    "such that `probs_merged[:, j] == \\sum_{maps[j']=j} probs[:, j']`. If any element in `maps` is negative (does not map to anything in the new set of classes), `probs_merged` is normalized such that each row sums up to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a3baf",
   "metadata": {},
   "source": [
    "## 5. Reducing from subword-level to word-level \n",
    "\n",
    "When a sentence gets tokenized, each word-level token may be broken down into subword-level tokens, each of which generates a predicted probability. Given that we only have the labels for word-level tokens, we reduce the subword-level tokens to word-level tokens. \n",
    "\n",
    "\\* For this example, most subwords-to-words reduction are handled internally, but for most other models the reduction has to be done manually. In the following, we show our method of reduction, which is slightly different from how the `bert` model reduces it. See [here](https://github.com/kamalkraj/BERT-NER/blob/dev/bert.py#L85) for more info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43366dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sentence:\\t' + sentences[0]) \n",
    "print('Given words:\\t' + str(given_words[0])) \n",
    "print('Subwords:\\t' + str(tokens)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eccd57",
   "metadata": {},
   "source": [
    "The word `lamb` is tokenized into two subwords `la` and `mb`. In this case, we assign the average predicted probabilities of the two subwords to the token. Alternatively, we can take the weighted average, such that the weight for each predicted probability is proportional to the length of its corresponding subword. This is to ensure that longer subwords have heavier weights on the average predicted probabilities, although the benefits are insignificant for most datasets. \n",
    "\n",
    "Each tokenizer tokenizes sentences differently. In some rare cases, a subword may overlap two tokens, resulting in a misalignment in tokenization. For example, consider the following tokenization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0565e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sentence = 'Massachusetts Institute of Technology (MIT)' \n",
    "demo_given_words = ['Massachusetts', 'Institute', 'of', 'Technology', '(', 'MIT', ')'] \n",
    "demo_subwords = ['Massachusetts', 'Institute', 'of', 'Technology', '(M', 'IT', ')'] \n",
    "\n",
    "print('Sentence:\\t' + demo_sentence) \n",
    "print('Given words:\\t' + str(demo_given_words)) \n",
    "print('Subwords:\\t' + str(demo_subwords)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a794f3d0",
   "metadata": {},
   "source": [
    "In this case, we assign the predicted probabilities of `(M` to `(`, and the average predicted probabilities of `(M` and `IT` to `MIT`. \n",
    "\n",
    "We use the method above to map the predicted probabilities for each token generated by the tokenizers to each token in the original dataset. `get_pred_probs` return `pred_probs` which is a nested list, such that `pred_probs[i]` is a `np.array` of shape `(N, K)`, such that `N` is the number of given tokens for sentence `i`, and `K` is the number of classes. This is the expected format for methods in the `cleanlab.token_classification` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3656f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = [get_pred_probs(sentence_probs[i], sentence_tokens[i], given_words[i]) \n",
    "                         for i in range(len(sentences))] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c583d",
   "metadata": {},
   "source": [
    "For example, we observe the tokens, given labels of the first sentence, and its predicted probabilities and label for each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ['O', 'MISC', 'PER', 'ORG', 'LOC'] \n",
    "for word, label, prob in zip(given_words[0], given_labels[0], pred_probs[0]): \n",
    "    print('Token: %s, given label: %s' % (word, entities[label])) \n",
    "    print('Predicted probabilities: %s' % str(np.round(prob, 7))) \n",
    "    print('Predicted label: %s\\n' % entities[np.argmax(prob)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb30126",
   "metadata": {},
   "source": [
    "## 6. Save `pred_probs` \n",
    "\n",
    "Lastly, we save the predicted probabilities and given labels. `to_dict` converts `pred_probs` into a dictionary `d` where `d[str(i)]==pred_probs[i]`. The dictionary is saved as a `.npz` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3066803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_dict = to_dict(pred_probs) \n",
    "np.savez('pred_probs.npz', **pred_probs_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2d51a",
   "metadata": {},
   "source": [
    "## 7. Model evaluation  \n",
    "\n",
    "Lastly, we evaluate the model accuracy. We use the definition of precision and recall introduced by CoNLL-2003: \n",
    "\n",
    "> *“precision is the percentage of named entities found by the learning system that are correct. Recall is the percentage of named entities present in the corpus that are found by the system. A named entity is correct only if it is an exact match of the corresponding entity in the data file.”*\n",
    "\n",
    "See [here](https://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/) for more info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d285325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [pred_prob.argmax(axis=1) for pred_prob in pred_probs] \n",
    "predictions_flatten = [pred for prediction in predictions for pred in prediction] \n",
    "given_labels_flatten = [label for given_label in given_labels for label in given_label] \n",
    "\n",
    "counts = [0, 0, 0, 0] \n",
    "correct = 0 \n",
    "\n",
    "for truth, prediction in zip(given_labels_flatten, predictions_flatten): \n",
    "    if truth != 0: \n",
    "        if truth == prediction: \n",
    "            counts[0] += 1 \n",
    "        counts[1] += 1 \n",
    "    if prediction != 0: \n",
    "        if truth == prediction: \n",
    "            counts[2] += 1 \n",
    "        counts[3] += 1 \n",
    "    if truth == prediction: \n",
    "        correct += 1 \n",
    "        \n",
    "precision = counts[2] / counts[3] \n",
    "recall = counts[0] / counts[1] \n",
    "f1 = 2 * precision * recall / (precision + recall) \n",
    "accuracy = correct / len(given_labels_flatten) \n",
    "\n",
    "balanced_accuracy = balanced_accuracy_score(given_labels_flatten, predictions_flatten) \n",
    "\n",
    "print('Precision\\t\\t%.3f\\nRecall\\t\\t\\t%.3f\\nf1-score\\t\\t%.3f\\nAccuracy\\t\\t%.3f\\nBalanced Accuracy\\t%.3f' % \n",
    "     (precision, recall, f1, accuracy, balanced_accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab910c",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "expected_words = ['Eu', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'] \n",
    "expected_labels = [3, 0, 1, 0, 0, 0, 1, 0, 0] \n",
    "if given_words[0] != expected_words or given_labels[0] != expected_labels: \n",
    "    raise Exception(\"Something wrong with reading file\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
