{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning for caltech256 using Cleanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cleanlab/examples/blob/master/active_learning_multiannotator/active_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a practical approach to active learning for training classification models with cleanlab. In active learning, we aim to construct a labeled dataset by collecting the fewest labels that still allow us to train an accurate classifier model. Here we assume data labeling is done in **batches**, and between these data labeling rounds, we retrain our classifier to decide what examples (i.e. datapoints) to label next round. We consider labeling a batch of examples per round and are limited to a single annotation per example.\n",
    "\n",
    "cleanlab provides an active learning score quantifying how desirable it is to collect an additional label for every possible unlabeled example\n",
    "\n",
    "This notebook demonstrates how to compute these easily for use in sequential active learning, showing how a classification model iteratively improves after labeling more examples for multiple rounds.\n",
    "This notebook implements the following steps:\n",
    "\n",
    "1. Establish some already labeled data. Use this data to train a classifier model and then obtain out-of-sample predicted probabilities for each labeled and unlabeled example.\n",
    "2. Compute active learning scores for every example, which estimate our current confidence in knowing its true label.\n",
    "3. Collect additional labels for the unlabeled examples with the lowest active learning scores. These are the most potentially informative examples whose true label we are least certain of. \n",
    "4. Repeat the steps above to collect as many labels as your budget permits.\n",
    "\n",
    "The accuracy of the model trained on the resulting dataset will generally match that of the same model trained on a much larger set of randomly collected labels. I.e. this is the most cost-effective way to train an accurate classifier!\n",
    "\n",
    "In this example we use the caltch256 dataset combined with autogluon's new MultiModalPredictor architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies and get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.multimodal import MultiModalPredictor\n",
    "from autogluon.vision import ImageDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cleanlab.multiannotator import get_majority_vote_label, get_label_quality_multiannotator, get_active_learning_scores\n",
    "from utils.model_training_autogluon import cross_val_predict_autogluon_classification_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the following datafiles:\n",
    "\n",
    "- `dataset` is a DataFrame that contains labels and image paths for each example\n",
    "\n",
    "We will then randomly split the dataset into train and test splits. Test data will just be used to measure the accuracy in our model after each active learning round. The train data will further be split into a labeled and unlabeled part. For the df_labeled we will use the labels to train the model while the df_unlabeled will simulate active learning by allowing us to artifically insert mode labeled data in between rounds.\n",
    "\n",
    "We will choose to start with num_labeled_per_class = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘256_ObjectCategories.zip’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/ActiveLearning/Caltech256/256_ObjectCategories.zip' && unzip -o -q 256_ObjectCategories.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset.from_folder('./256_ObjectCategories/')\n",
    "dataset = dataset.replace(257, 256) # no class class in dataset is labeled as 257, we need to reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train-test split\n",
    "X, y = np.arange(len(dataset) * 2).reshape((len(dataset), 2)), range(len(dataset))\n",
    "_, _, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "df_train = dataset.iloc[y_train]\n",
    "df_test = dataset.iloc[y_test]\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(dataset, test_size=0.33, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled(dataset,  num_labeled_per_class=15):\n",
    "    \"\"\"Splits provided dataset into two datasets. With df_labeled containing num_labeled_per_class labeles for \n",
    "    each class and df_unlabeled containing the rest of the rows in dataset\"\"\"\n",
    "    \n",
    "    df_labeled = dataset.groupby(\"label\").sample(n=num_labeled_per_class, random_state=123)\n",
    "    labeled_index = list(df_labeled.index)\n",
    "    unlabeled_index = [i for i in range(len(dataset)) if i not in labeled_index]\n",
    "    df_unlabeled = dataset.iloc[unlabeled_index]\n",
    "    df_unlabeled = df_unlabeled.reset_index(drop=True)\n",
    "    df_labeled = df_labeled.reset_index(drop=True)    \n",
    "    return labeled_index, df_labeled, unlabeled_index, df_unlabeled\n",
    "\n",
    "labeled_index, df_labeled, unlabeled_index, df_unlabeled = get_labeled(dataset, num_labeled_per_class=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model to obtain predicted probabilites\n",
    "\n",
    "First, we train our model on a set of labels obtained by `get_labeled` to get the out-of-sample predicted class probabilities for both the labeled and unlabeled data. \n",
    "\n",
    "The train function will return two sets of predicted probabilites, `pred_probs_labeled` are the predicted probabilites for examples that have existing annotator labels (they correspond directly with the rows in `df_labeled`), whereas `pred_probs_unlabeled` are the predicted probabilites for examples that do not have any annotator labels (they correspond directly with the rows in `df_unlabeled`). These predicted probabilities will later be used to compute the active learning score.\n",
    "\n",
    "If working with your own dataset, you should consider modifying this `cross_val_predict_autogluon_classification_dataset` function so that it is better fitted for training your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230315_210329/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Running Cross-Validation on Split: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "pred_probs_labeled, pred_probs_unlabeled, labels_labeled, images = cross_val_predict_autogluon_classification_dataset(\n",
    "                                                                                    dataset,\n",
    "                                                                                    out_folder=None,\n",
    "                                                                                    cv_n_folds=3,\n",
    "                                                                                    df_predict=df_unlabeled,\n",
    "                                                                                    hypterparameters={},\n",
    "                                                                                    time_limit=180,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain active learning scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get the active learning scores for each datapoint (both labeled and unlabeled) by using a combination of the annotators' agremeent and model confidence. These scores represent how confident we are about an example's true label based on the currently obtained annotations; examples with the lowest scores are those for which additional labels should be collected (i.e. likely the most informative). These scores are estimated via an **Active CROWDLAB** algorithm developed by the Cleanlab team, and may sometimes prioritize an already-labeled example over an unlabeled example if the annotations for the labeled example are deemed unreliable (Active CROWDLAB appropriately estimates the value of collecting new annotations for unlabeled data vs already-labeled data). \n",
    "\n",
    "Since we only have a single label for each datapoint, we only consider scores for previously unlabeled examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute active learning scores\n",
    "active_learning_scores, active_learning_scores_unlabeled = get_active_learning_scores(\n",
    "    labels_labeled, pred_probs_labeled, pred_probs_unlabeled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of active learning scores\n",
    "active_learning_scores[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get index to relabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can ranks the examples by their active learning scores, and obtain the index of the examples with the lowest scores; these are the least confident examples which we will want to collect more labels for.\n",
    "\n",
    "The code cell below shows how to obtain their respective indices to collect more labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_to_label(active_learning_scores_unlabeled, batch_size_to_label):\n",
    "    \"\"\"Function to get indices of examples with the lowest active learning score to collect more labels for.\"\"\"\n",
    "    return np.argsort(active_learning_scores_unlabeled)[:batch_size_to_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_to_label = 100 # you can pick how many examples to collect more labels for at each round\n",
    "\n",
    "# get next idx to label based on batch_size_to_label and magnitude of each example's active learning score\n",
    "next_idx_to_label = get_idx_to_label(active_learning_scores_unlabeled, batch_size_to_label=batch_size_to_label)\n",
    "next_idx_to_label[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving model accuracy over 15 rounds of active learning (collecting new labels) \n",
    "\n",
    "The code below shows a full demonstration of how we can repeatedly use the functions demonstrated above for multiple rounds in order to select which examples to collect new labels for, ask annotators to provide these new labels (via a noisy simulation in this example), and use the newly collected labels to train an improved classification model.\n",
    "\n",
    "This demonstration runs this active learning loop for 15 rounds, choosing 100 examples to collect more labels for each round. Each round, we use labeled examples to train a classifier (here we used autogluon's `MultiModalPredictor` classifier) and obtain out-of-sample predicted probabilites, which are then used to compute the active learning scores for every example. We then synthetically collect new labels (this process is meant to simulate getting a new annotator to annotated a selection of examples) and repeat the active learning loop. \n",
    "\n",
    "[Optional step] We also measure the model performance on a test set each round to demonstrate the improvement of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_next_iter_data(df_labeled, df_unlabeled, relabel_idx_unlabeled):\n",
    "    \"\"\"Updates inputs after additional labels have been collected in a single active learning round,\n",
    "    this ensures that the inputs will be well formatted for the next round of active learning.\"\"\"\n",
    "\n",
    "    df_labeled = pd.concat([df_labeled,df_unlabeled.iloc[relabel_idx_unlabeled]], ignore_index=True)\n",
    "    df_unlabeled = df_unlabeled.drop(relabel_idx_unlabeled)\n",
    "    df_unlabeled = df_unlabeled.reset_index(drop=True)\n",
    "    df_labeled = df_labeled.reset_index(drop=True)  \n",
    "    return df_labeled, df_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 30\n",
    "batch_size_to_label = 100\n",
    "hypterparameters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuacy_arr = np.full(num_rounds, np.nan)\n",
    "\n",
    "for i in range(num_rounds):\n",
    "    # train model to get out-of-sample predicted probabilites\n",
    "    out_folder = None\n",
    "    \n",
    "    print('fitting xval model')\n",
    "    pred_probs_labeled, pred_probs_unlabeled, labels_labeled, images = cross_val_predict_autogluon_classification_dataset(df_labeled,\n",
    "                                                                                    out_folder=out_folder,\n",
    "                                                                                    cv_n_folds=3,\n",
    "                                                                                    df_predict=df_unlabeled,\n",
    "                                                                                    hypterparameters=hypterparameters,)\n",
    "    # train a model on the full set of labeled data to evaluate model accuracy for the current round,\n",
    "    # this is an optional step for demonstration purposes, in practical applications \n",
    "    # you may not have ground truth labels\n",
    "    print('fitting full model')\n",
    "    predictor = MultiModalPredictor(label=\"label\", path=None, problem_type=\"classification\",warn_if_exist=False)\n",
    "    # train model on train indices in this split\n",
    "    predictor.fit(\n",
    "        train_data=df_labeled,\n",
    "        hyperparameters=hypterparameters,\n",
    "    )\n",
    "    # predicted probabilities for test split\n",
    "    pred_labels = predictor.predict(data=df_test)\n",
    "    true_labels_test = np.array(df_test['label'].tolist())\n",
    "    model_accuacy_arr[i] = np.mean(pred_labels == true_labels_test)\n",
    "    \n",
    "    print('test round: ', i, 'accuracy: ', np.mean(pred_labels == true_labels_test))\n",
    "        \n",
    "    print('computing active learning scores')\n",
    "    # compute active learning scores\n",
    "    active_learning_scores, active_learning_scores_unlabeled = get_active_learning_scores(\n",
    "        labels_labeled, pred_probs_labeled, pred_probs_unlabeled\n",
    "    )\n",
    "    \n",
    "    print('getting idx to relabel')\n",
    "    # get the indices of examples to collect more labels for\n",
    "    relabel_idx_unlabeled = get_idx_to_label(\n",
    "        active_learning_scores_unlabeled=active_learning_scores_unlabeled,\n",
    "        batch_size_to_label=batch_size_to_label,\n",
    "    )\n",
    "    \n",
    "    relabel_idx_unlabeled = np.random.choice(range(df_unlabeled.shape[0]), batch_size_to_label, replace=False)\n",
    "    print('setting up next iter')\n",
    "    # format the data for the next round of active learning, ie. moving some unlabeled \n",
    "    # examples to the labeled pool because we are collecting labels for them\n",
    "    df_labeled, df_unlabeled = setup_next_iter_data(df_labeled, df_unlabeled, relabel_idx_unlabeled)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial model test accuracy: {model_accuacy_arr[0]:.3}\")\n",
    "print(f\"Final model test accuracy (after 15 rounds of active learning): {model_accuacy_arr[-1]:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"model_acc_30_rounds_activelab\", model_accuacy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_accuacy_arr)\n",
    "plt.xticks(range(num_rounds))\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Model Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('model_acc_30_rounds_activelab.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see that the model accuracy increases steadily with each additional round of collecting more labels, getting improved consensus labels, and model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "00885e89789f58e60dbba52a405dc834aaf92411914fde0d391f9b48289a0610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
