{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning with Autogluon\n",
    "Train accurate classifier models with minimal data labeling (and minimal code) via active learning and AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cleanlab/examples/blob/master/active_learning_single_annotator/active_learning_single_annotator.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a practical approach to active learning for training an accurate image classifier with AutoGluon and cleanlab. We consider standard active learning settings with a pool of unlabeled examples, where we label a batch of examples at a time and collect **at most one label** per example. \n",
    "\n",
    "In **Active Learning**, we aim to construct a labeled dataset by collecting the fewest labels that still allow us to train an accurate classifier model. Here we assume data labeling is done in **batches**, and between these data labeling rounds, we retrain our classifier to decide what previously unlabeled examples (i.e. datapoints) to label next round.\n",
    "\n",
    "\n",
    "This notebook demonstrates how to compute these scores easily for use in sequential active learning, showing how a classification model iteratively improves after labeling more examples for multiple rounds with the following steps:\n",
    "\n",
    "1. Establish an initially labeled dataset, `df_labeled` to train the model on. This is a small subset of our training data, `df_train`. The rest of the training data is marked as `df_unlabeled`.\n",
    "2. Train the model on the labeled data and get predictions for the unlabeled data, `pred_probs_unlabeled`.\n",
    "3. Compute active learning scores for all unlabeled examples and select which samples to add to the dataset.\n",
    "4. Add the selected samples from `df_unlabeld` into `df_labeled`.\n",
    "5. Repeat the steps above to collect as many labels as your budget permits.\n",
    "\n",
    "The accuracy of the model trained on the resulting dataset will generally match that of the same model trained on a much larger set of randomly collected labels i.e. this is the most cost-effective way to train an accurate classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies and data\n",
    "\n",
    "In this example we use the [Caltech-256](https://data.caltech.edu/records/nyy15-4j048)[1] image classification dataset. Any dataset in the same format can be substituted instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.multimodal import MultiModalPredictor\n",
    "from gluoncv.auto.data.dataset import ImageClassificationDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cleanlab.multiannotator import get_label_quality_scores, get_active_learning_scores\n",
    "from utils.model_training_autogluon import predict_autogluon_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘256_ObjectCategories.zip’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/ActiveLearning/Caltech256/256_ObjectCategories.zip' && unzip -o -q 256_ObjectCategories.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select initial labeled dataset\n",
    "We load the following datafiles:\n",
    "\n",
    "- `dataset` is a DataFrame that contains labels and file paths for each image (i.e. example) from Caltech-256\n",
    "\n",
    "We then randomly split the dataset into train and test splits. Test data are just used to measure the accuracy in our model after each active learning round (you may not have this in your applications).  The train data will further be split into labeled and unlabeled pools. For the `df_labeled` we will use the labels to train the model while the `df_unlabeled` will simulate active learning by allowing us to artificially insert more labeled data in between rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageClassificationDataset.from_folder('./256_ObjectCategories/')\n",
    "dataset = dataset.replace(257, 256) # no class class in dataset is labeled as 257, we need to reindex\n",
    "\n",
    "# Split data into train and test\n",
    "df_train, df_test = train_test_split(dataset, test_size=0.33, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data will further be split into a labeled and unlabeled part, `df_labeled` and `df_unlabeled` respectively. \n",
    "\n",
    "We will use the labels in `df_labeled` to train the model while `df_unlabeled` will simulate active learning by allowing us to artificially insert more labeled data in between rounds.\n",
    "\n",
    "We will arbitrarily choose to start with `num_labeled_per_class = 8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled(dataset,  num_labeled_per_class=8):\n",
    "    \"\"\"Splits provided dataset into two datasets. With df_labeled containing num_labeled_per_class labeles for \n",
    "    each class and df_unlabeled containing the rest of the rows in dataset\"\"\"\n",
    "    \n",
    "    df_labeled = dataset.groupby(\"label\").sample(n=num_labeled_per_class, random_state=123)\n",
    "    labeled_index = list(df_labeled.index)\n",
    "    unlabeled_index = [i for i in range(len(dataset)) if i not in labeled_index]\n",
    "    df_unlabeled = dataset.iloc[unlabeled_index]\n",
    "    df_unlabeled = df_unlabeled.reset_index(drop=True)\n",
    "    df_labeled = df_labeled.reset_index(drop=True)    \n",
    "    return df_labeled, df_unlabeled\n",
    "\n",
    "# Split the train data into labeled and unlabeled with 8 labeled per each class\n",
    "df_labeled, df_unlabeled = get_labeled(df_train, num_labeled_per_class=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on labeled data & obtain predicted probabilites for unlabeled data\n",
    "\n",
    "First, we train our model on the labeled data obtained from `get_labeled` and get the probabilities for the unlabeled data. The train function returns our `predictor` fitted to `df_labeled` and `pred_probs_unlabeled` which are the predicted probabilities for examples that do not have any annotator labels (they correspond directly with the rows in `df_unlabeled`). These predicted probabilities will later be used to compute the active learning score.\n",
    "\n",
    "If working with your own model, you should consider modifying this `predict_autogluon_classification` function so that it is better fitted for training your specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230317_201329/\"\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Auto select gpus: [0]\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                            | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | model             | TimmAutoModelForImagePrediction | 87.0 M\n",
      "1 | validation_metric | Accuracy                        | 0     \n",
      "2 | loss_func         | CrossEntropyLoss                | 0     \n",
      "----------------------------------------------------------------------\n",
      "87.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.0 M    Total params\n",
      "174.013   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  33%|█████████████████████████▉                                                    | 103/310 [00:13<00:27,  7.60it/s, loss=5.58, v_num=]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                         | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                            | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  34%|██████████████████████████▏                                                   | 104/310 [00:13<00:27,  7.54it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  34%|██████████████████████████▍                                                   | 105/310 [00:13<00:27,  7.59it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  34%|██████████████████████████▋                                                   | 106/310 [00:13<00:26,  7.63it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  35%|██████████████████████████▉                                                   | 107/310 [00:13<00:26,  7.67it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  35%|███████████████████████████▏                                                  | 108/310 [00:13<00:26,  7.72it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  35%|███████████████████████████▍                                                  | 109/310 [00:14<00:25,  7.76it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  35%|███████████████████████████▋                                                  | 110/310 [00:14<00:25,  7.80it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  36%|███████████████████████████▉                                                  | 111/310 [00:14<00:25,  7.85it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  36%|████████████████████████████▏                                                 | 112/310 [00:14<00:25,  7.89it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  36%|████████████████████████████▍                                                 | 113/310 [00:14<00:24,  7.93it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  37%|████████████████████████████▋                                                 | 114/310 [00:14<00:24,  7.97it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  37%|████████████████████████████▉                                                 | 115/310 [00:14<00:24,  8.01it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  37%|█████████████████████████████▏                                                | 116/310 [00:14<00:24,  8.06it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  38%|█████████████████████████████▍                                                | 117/310 [00:14<00:23,  8.10it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  38%|█████████████████████████████▋                                                | 118/310 [00:14<00:23,  8.14it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  38%|█████████████████████████████▉                                                | 119/310 [00:14<00:23,  8.18it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  39%|██████████████████████████████▏                                               | 120/310 [00:14<00:23,  8.22it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  39%|██████████████████████████████▍                                               | 121/310 [00:14<00:22,  8.26it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  39%|██████████████████████████████▋                                               | 122/310 [00:14<00:22,  8.30it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  40%|██████████████████████████████▉                                               | 123/310 [00:14<00:22,  8.34it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  40%|███████████████████████████████▏                                              | 124/310 [00:14<00:22,  8.38it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  40%|███████████████████████████████▍                                              | 125/310 [00:14<00:21,  8.42it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  41%|███████████████████████████████▋                                              | 126/310 [00:14<00:21,  8.46it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  41%|███████████████████████████████▉                                              | 127/310 [00:14<00:21,  8.49it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  41%|████████████████████████████████▏                                             | 128/310 [00:15<00:21,  8.53it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  42%|████████████████████████████████▍                                             | 129/310 [00:15<00:21,  8.57it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  42%|████████████████████████████████▋                                             | 130/310 [00:15<00:20,  8.61it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  42%|████████████████████████████████▉                                             | 131/310 [00:15<00:20,  8.65it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  43%|█████████████████████████████████▏                                            | 132/310 [00:15<00:20,  8.68it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  43%|█████████████████████████████████▍                                            | 133/310 [00:15<00:20,  8.72it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  43%|█████████████████████████████████▋                                            | 134/310 [00:15<00:20,  8.76it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  44%|█████████████████████████████████▉                                            | 135/310 [00:15<00:19,  8.79it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  44%|██████████████████████████████████▏                                           | 136/310 [00:15<00:19,  8.83it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  44%|██████████████████████████████████▍                                           | 137/310 [00:15<00:19,  8.87it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  45%|██████████████████████████████████▋                                           | 138/310 [00:15<00:19,  8.90it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  45%|██████████████████████████████████▉                                           | 139/310 [00:15<00:19,  8.94it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  45%|███████████████████████████████████▏                                          | 140/310 [00:15<00:18,  8.97it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  45%|███████████████████████████████████▍                                          | 141/310 [00:15<00:18,  9.01it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  46%|███████████████████████████████████▋                                          | 142/310 [00:15<00:18,  9.04it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  46%|███████████████████████████████████▉                                          | 143/310 [00:15<00:18,  9.08it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  46%|████████████████████████████████████▏                                         | 144/310 [00:15<00:18,  9.11it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  47%|████████████████████████████████████▍                                         | 145/310 [00:15<00:18,  9.14it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  47%|████████████████████████████████████▋                                         | 146/310 [00:15<00:17,  9.17it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  47%|████████████████████████████████████▉                                         | 147/310 [00:15<00:17,  9.21it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  48%|█████████████████████████████████████▏                                        | 148/310 [00:16<00:17,  9.24it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  48%|█████████████████████████████████████▍                                        | 149/310 [00:16<00:17,  9.27it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  48%|█████████████████████████████████████▋                                        | 150/310 [00:16<00:17,  9.31it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  49%|█████████████████████████████████████▉                                        | 151/310 [00:16<00:17,  9.34it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  49%|██████████████████████████████████████▏                                       | 152/310 [00:16<00:16,  9.37it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  49%|██████████████████████████████████████▍                                       | 153/310 [00:16<00:16,  9.41it/s, loss=5.58, v_num=]\u001b[A\n",
      "Epoch 0:  50%|██████████████████████████████████████▋                                       | 154/310 [00:16<00:16,  9.44it/s, loss=5.58, v_num=]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|███████████████████████████████████████                                       | 155/310 [00:16<00:16,  9.48it/s, loss=5.58, v_num=]\u001b[A\n",
      "                                                                                                                                                 \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 6: 'val_accuracy' reached 0.00728 (best 0.00728), saving model to '/home/ubuntu/examples/active_learning_single_annotator/AutogluonModels/ag-20230317_201329/epoch=0-step=6.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  70%|██████████████████████████████████████████████████████▊                       | 218/310 [00:29<00:12,  7.29it/s, loss=5.47, v_num=]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time limit reached. Elapsed time is 0:00:30. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  71%|███████████████████████████████████████████████████████                       | 219/310 [00:30<00:12,  7.29it/s, loss=5.49, v_num=]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                                         | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                                            | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  71%|███████████████████████████████████████████████████████▎                      | 220/310 [00:30<00:12,  7.27it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  71%|███████████████████████████████████████████████████████▌                      | 221/310 [00:30<00:12,  7.29it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  72%|███████████████████████████████████████████████████████▊                      | 222/310 [00:30<00:12,  7.31it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  72%|████████████████████████████████████████████████████████                      | 223/310 [00:30<00:11,  7.33it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  72%|████████████████████████████████████████████████████████▎                     | 224/310 [00:30<00:11,  7.35it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  73%|████████████████████████████████████████████████████████▌                     | 225/310 [00:30<00:11,  7.37it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  73%|████████████████████████████████████████████████████████▊                     | 226/310 [00:30<00:11,  7.39it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  73%|█████████████████████████████████████████████████████████                     | 227/310 [00:30<00:11,  7.42it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  74%|█████████████████████████████████████████████████████████▎                    | 228/310 [00:30<00:11,  7.44it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  74%|█████████████████████████████████████████████████████████▌                    | 229/310 [00:30<00:10,  7.46it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  74%|█████████████████████████████████████████████████████████▊                    | 230/310 [00:30<00:10,  7.48it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  75%|██████████████████████████████████████████████████████████                    | 231/310 [00:30<00:10,  7.50it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  75%|██████████████████████████████████████████████████████████▎                   | 232/310 [00:30<00:10,  7.52it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  75%|██████████████████████████████████████████████████████████▋                   | 233/310 [00:30<00:10,  7.54it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  75%|██████████████████████████████████████████████████████████▉                   | 234/310 [00:30<00:10,  7.56it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  76%|███████████████████████████████████████████████████████████▏                  | 235/310 [00:31<00:09,  7.58it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  76%|███████████████████████████████████████████████████████████▍                  | 236/310 [00:31<00:09,  7.60it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  76%|███████████████████████████████████████████████████████████▋                  | 237/310 [00:31<00:09,  7.62it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  77%|███████████████████████████████████████████████████████████▉                  | 238/310 [00:31<00:09,  7.64it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  77%|████████████████████████████████████████████████████████████▏                 | 239/310 [00:31<00:09,  7.66it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  77%|████████████████████████████████████████████████████████████▍                 | 240/310 [00:31<00:09,  7.68it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  78%|████████████████████████████████████████████████████████████▋                 | 241/310 [00:31<00:08,  7.69it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  78%|████████████████████████████████████████████████████████████▉                 | 242/310 [00:31<00:08,  7.71it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  78%|█████████████████████████████████████████████████████████████▏                | 243/310 [00:31<00:08,  7.73it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  79%|█████████████████████████████████████████████████████████████▍                | 244/310 [00:31<00:08,  7.75it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  79%|█████████████████████████████████████████████████████████████▋                | 245/310 [00:31<00:08,  7.77it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  79%|█████████████████████████████████████████████████████████████▉                | 246/310 [00:31<00:08,  7.79it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  80%|██████████████████████████████████████████████████████████████▏               | 247/310 [00:31<00:08,  7.81it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  80%|██████████████████████████████████████████████████████████████▍               | 248/310 [00:31<00:07,  7.83it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  80%|██████████████████████████████████████████████████████████████▋               | 249/310 [00:31<00:07,  7.85it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  81%|██████████████████████████████████████████████████████████████▉               | 250/310 [00:31<00:07,  7.87it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  81%|███████████████████████████████████████████████████████████████▏              | 251/310 [00:31<00:07,  7.89it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  81%|███████████████████████████████████████████████████████████████▍              | 252/310 [00:31<00:07,  7.90it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████████████████████████████████████████▋              | 253/310 [00:31<00:07,  7.92it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  82%|███████████████████████████████████████████████████████████████▉              | 254/310 [00:31<00:07,  7.94it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  82%|████████████████████████████████████████████████████████████████▏             | 255/310 [00:32<00:06,  7.96it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  83%|████████████████████████████████████████████████████████████████▍             | 256/310 [00:32<00:06,  7.98it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  83%|████████████████████████████████████████████████████████████████▋             | 257/310 [00:32<00:06,  8.00it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  83%|████████████████████████████████████████████████████████████████▉             | 258/310 [00:32<00:06,  8.02it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  84%|█████████████████████████████████████████████████████████████████▏            | 259/310 [00:32<00:06,  8.03it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  84%|█████████████████████████████████████████████████████████████████▍            | 260/310 [00:32<00:06,  8.05it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  84%|█████████████████████████████████████████████████████████████████▋            | 261/310 [00:32<00:06,  8.07it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  85%|█████████████████████████████████████████████████████████████████▉            | 262/310 [00:32<00:05,  8.09it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  85%|██████████████████████████████████████████████████████████████████▏           | 263/310 [00:32<00:05,  8.11it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  85%|██████████████████████████████████████████████████████████████████▍           | 264/310 [00:32<00:05,  8.12it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  85%|██████████████████████████████████████████████████████████████████▋           | 265/310 [00:32<00:05,  8.14it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  86%|██████████████████████████████████████████████████████████████████▉           | 266/310 [00:32<00:05,  8.16it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  86%|███████████████████████████████████████████████████████████████████▏          | 267/310 [00:32<00:05,  8.18it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  86%|███████████████████████████████████████████████████████████████████▍          | 268/310 [00:32<00:05,  8.20it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  87%|███████████████████████████████████████████████████████████████████▋          | 269/310 [00:32<00:04,  8.21it/s, loss=5.49, v_num=]\u001b[A\n",
      "Epoch 0:  87%|███████████████████████████████████████████████████████████████████▉          | 270/310 [00:32<00:04,  8.23it/s, loss=5.49, v_num=]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  87%|████████████████████████████████████████████████████████████████████▏         | 271/310 [00:32<00:04,  8.25it/s, loss=5.49, v_num=]\u001b[A\n",
      "                                                                                                                                                 \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 10: 'val_accuracy' reached 0.01699 (best 0.01699), saving model to '/home/ubuntu/examples/active_learning_single_annotator/AutogluonModels/ag-20230317_201329/epoch=0-step=10.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  87%|████████████████████████████████████████████████████████████████████▏         | 271/310 [00:48<00:07,  5.55it/s, loss=5.49, v_num=]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:automm:Start to fuse 2 checkpoints via the greedy soup algorithm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  5.89it/s]\n",
      "Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  5.80it/s]\n",
      "Predicting DataLoader 0: 100%|███████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:automm:Models and intermediate outputs are saved to /home/ubuntu/examples/active_learning_single_annotator/AutogluonModels/ag-20230317_201329 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|████████████████████████████████████████████████████████████████████████████████▋| 595/597 [01:40<00:00,  5.90it/s]"
     ]
    }
   ],
   "source": [
    "predictor, pred_probs_unlabeled = predict_autogluon_classification( df_labeled,\n",
    "                                                                    out_folder=None,\n",
    "                                                                    df_predict=df_unlabeled,\n",
    "                                                                    time_limit=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain active learning scores\n",
    "\n",
    "Next, we will compute active learning scores that estimate the informativeness of labeling each datapoint. Since we will collect at most one annotation per example, we only care about active learning scores for the unlabeled data: `active_learning_scores_unlabeled`. \n",
    "\n",
    "These scores represent how confident we are about an example's true label based on the currently obtained annotations; examples with the lowest scores are those for which additional labels should be collected (i.e. likely the most informative). These scores are estimated via [ActiveLab](https://arxiv.org/abs/2301.11856), an algorithm developed by the Cleanlab team. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack active learning multiannotator with dummy pred_probs\n",
    "dummy_pred_probs = np.zeros((df_labeled.shape[0], pred_probs_unlabeled.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute active learning scores\n",
    "_, active_learning_scores_unlabeled = get_active_learning_scores(\n",
    "    df_labeled['label'].to_numpy(), dummy_pred_probs, pred_probs_unlabeled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of active learning scores\n",
    "active_learning_scores_unlabeled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get index to relabel\n",
    "\n",
    "Lastly, we can rank the examples by their active learning scores, and obtain the index of the examples with the lowest scores; these are the **unlabeled** examples whose true label our current model is least confident about. We will want to prioritize these examples for labeling next.\n",
    "\n",
    "The code cell below shows how to obtain their respective indices in order to collect labels for these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_to_label(active_learning_scores_unlabeled, batch_size_to_label):\n",
    "    \"\"\"Function to get indices of examples with the lowest active learning score to collect more labels for.\"\"\"\n",
    "    \n",
    "    return np.argsort(active_learning_scores_unlabeled)[:batch_size_to_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_to_label = 100 # you can pick how many examples to collect more labels for at each round\n",
    "\n",
    "# get next idx to label based on batch_size_to_label and magnitude of each example's active learning score\n",
    "next_idx_to_label = get_idx_to_label(active_learning_scores_unlabeled, batch_size_to_label=batch_size_to_label)\n",
    "next_idx_to_label[:5],active_learning_scores_unlabeled[next_idx_to_label[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving model accuracy over 10 rounds of active learning (collecting new labels) \n",
    "\n",
    "The code below shows a full demonstration of how we can repeatedly use the functions demonstrated above for multiple rounds in order to select which examples to collect labels for and use the newly collected labels to train an improved classification model.\n",
    "\n",
    "\n",
    "This demonstration runs this active learning loop for 10 rounds, choosing 100 new unlabeled examples to collect more labels for each round. Each round, we use labeled examples to train a classifier (here we used autogluon's `MultiModalPredictor` classifier) and obtain predicted probabilities for the unlabeled data, which are then used to compute the active learning scores for every example. We then synthetically collect new labels (this process is meant to simulate getting annotations for a selection of examples) using `setup_next_iter_data` and repeat the active learning loop. \n",
    "\n",
    "[Optional step] We also measure the model performance on a test set each round to demonstrate the improvement of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_next_iter_data(df_labeled, df_unlabeled, relabel_idx_unlabeled):\n",
    "    \"\"\"Updates inputs after additional labels have been collected in a single active learning round,\n",
    "    this ensures that the inputs will be well formatted for the next round of active learning.\"\"\"\n",
    "\n",
    "    df_labeled = pd.concat([df_labeled,df_unlabeled.iloc[relabel_idx_unlabeled]], ignore_index=True)\n",
    "    df_unlabeled = df_unlabeled.drop(relabel_idx_unlabeled)\n",
    "    df_unlabeled = df_unlabeled.reset_index(drop=True)\n",
    "    df_labeled = df_labeled.reset_index(drop=True)  \n",
    "    return df_labeled, df_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 5\n",
    "batch_size_to_label = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuacy_arr = np.full(num_rounds, np.nan)\n",
    "\n",
    "for i in range(num_rounds):\n",
    "    # train model to get out-of-sample predicted probabilites    \n",
    "    print('fitting model')\n",
    "    predictor, pred_probs_unlabeled = predict_autogluon_classification( df_labeled,\n",
    "                                                                        out_folder=None,\n",
    "                                                                        df_predict=df_unlabeled,\n",
    "                                                                        time_limit=30)\n",
    "    # train a model on the full set of labeled data to evaluate model accuracy for the current round,\n",
    "    # this is an optional step for demonstration purposes, in practical applications \n",
    "    # you may not have ground truth labels\n",
    "    print('predicting probabilities for test split')\n",
    "    pred_labels = predictor.predict(data=df_test)\n",
    "    true_labels_test = np.array(df_test['label'].tolist())\n",
    "    model_accuacy_arr[i] = np.mean(pred_labels == true_labels_test)\n",
    "    print('test round: ', i, 'accuracy: ', np.mean(pred_labels == true_labels_test))\n",
    "        \n",
    "    print('computing active learning scores')\n",
    "    # compute active learning scores\n",
    "    dummy_pred_probs = np.zeros((df_labeled.shape[0], pred_probs_unlabeled.shape[1]))\n",
    "    _, active_learning_scores_unlabeled = get_active_learning_scores(\n",
    "        df_labeled['label'].to_numpy(), dummy_pred_probs, pred_probs_unlabeled\n",
    "    )\n",
    "    \n",
    "    print('getting idx to relabel')\n",
    "    # get the indices of examples to collect more labels for\n",
    "    relabel_idx_unlabeled = get_idx_to_label(\n",
    "        active_learning_scores_unlabeled=active_learning_scores_unlabeled,\n",
    "        batch_size_to_label=batch_size_to_label,\n",
    "    )\n",
    "    \n",
    "    print('setting up next iter')\n",
    "    # format the data for the next round of active learning, ie. moving some unlabeled \n",
    "    # examples to the labeled pool because we are collecting labels for them\n",
    "    df_labeled, df_unlabeled = setup_next_iter_data(df_labeled, df_unlabeled, relabel_idx_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "\n",
    "From the plot below, we can see that the model accuracy increases steadily with each additional round of collecting more labels and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial model test accuracy: {model_accuacy_arr[0]:.3}\")\n",
    "print(f\"Final model test accuracy (after 15 rounds of active learning): {model_accuacy_arr[-1]:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"model_acc_30_rounds_activelab\", model_accuacy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_accuacy_arr)\n",
    "plt.xticks(range(num_rounds))\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Model Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('model_acc_30_rounds_activelab.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Griffin, G., Holub, A., & Perona, P. (2022). Caltech 256 (1.0) [Data set]. CaltechDATA. https://doi.org/10.22002/D1.20087"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "00885e89789f58e60dbba52a405dc834aaf92411914fde0d391f9b48289a0610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
