{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f08ac68",
   "metadata": {},
   "source": [
    "# Itteratively Find Improved Consensus Labels for Multiannotator Data using Cleanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0555b",
   "metadata": {},
   "source": [
    "This example shows how to improve consensus labels by combining the CROWDLAB algorithm with itteratively retraining a model. For an introductory tutorial on finding consensus labels with the multiannotator library see [Find Best Consensus Labels for Multiannotator Data using Cleanlab](linke to hui wen's). \n",
    "\n",
    "The following code uses the [cifar10h](linke cifar10h) multiannotator labeling dataset which is a collection of 2751 annotators each labeling 200 examples for all 10,000 test images of the original [cifar10](link cifar10) dataset but **any multiannotator classification image dataset should work with the code below**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95724ad1",
   "metadata": {},
   "source": [
    "## 1. Install and import required dependencies, build example folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b291ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.model_training import train_model\n",
    "from utils.model_training import sum_xval_folds\n",
    "from cleanlab.multiannotator import get_majority_vote_label\n",
    "from cleanlab.multiannotator import get_label_quality_multiannotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3551b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now() # Current date and time\n",
    "experiment_path = \"./experiment_\" + str(int(now.timestamp()))\n",
    "\n",
    "if not os.path.exists(experiment_path):\n",
    "    os.makedirs(experiment_path)\n",
    "    print(\"Directory \" , experiment_path ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , experiment_path ,  \" already exists\")\n",
    "\n",
    "print(f'Experiment saved in {experiment_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c50596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cifar10 data for model training or download it yourself by calling cifar2png cifar10 ./data/cifar10 --name-with-batch-index\n",
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/Multiannotator/cifar-10/cifar10_test.tar.gz'\n",
    "!tar -xzf cifar10_test.tar.gz\n",
    "\n",
    "# Import cifar10h pre calculated multiannotator labels and image paths\n",
    "!cd $experiment_path && wget -nc 'https://cleanlab-public.s3.amazonaws.com/Multiannotator/cifar-10h/cifar-10h-worst25-coin20/c10h_labels_worst25_coin20.npy'\n",
    "!cd $experiment_path && wget -nc 'https://cleanlab-public.s3.amazonaws.com/Multiannotator/cifar-10h/cifar-10h-worst25-coin20/c10h_image_paths.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2bed92",
   "metadata": {},
   "source": [
    "## 2. Load multiannotator labels and generate consensus labels for them\n",
    "`multiannotator_labels` for this example is a precalculated subset of the original `cifar10h` annotator labels. The subset takes the worst 25 annotators and the incrementally add annotators from worst to best if they share annotations with those already in the subset until each of the 10,000 examples has at least 1 annotation. \n",
    "\n",
    "The reason for this being `cifar10` is an unnaturally easy dataset to label and using the original dataset would make the annotator agreement too high for our method to contribute meaningful improvement. Additionally in practice it is rare to have 50 annotators annotate a specific example but this subset ensures significantly sparser annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "multiannotator_labels = np.load(f'{experiment_path}/c10h_labels_worst25_coin20.npy')\n",
    "\n",
    "# Load and reformat image paths to work for specific machine\n",
    "image_paths = np.load(f'{experiment_path}/c10h_image_paths.npy', allow_pickle=True)\n",
    "path = os.getcwd()\n",
    "image_paths = [f\"{path}/{image_path}\" for image_path in image_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a322a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Bringing Your Own Data (BYOD)?\n",
    "\n",
    "You can easily replace the above with your own multiannotator dataset, and continue with the rest of the example.\n",
    "\n",
    "`multiannotator_labels` should be a numpy array or pandas DataFrame with each column representing an annotator and each row representing an example. Your classes (and entries of `multiannotator_labels`) should be represented as integer indices 0, 1, ..., num_classes - 1, where examples that are not annotated by a particular annotator are represented using `np.nan`.\n",
    "    \n",
    "If working with images, `image_paths` should be a string of absolute or relative paths to the where each index corresponds the example for that row of `multiannotator_labels`.\n",
    "\n",
    "If working with other data, `image_paths` should be a string of examples corresponding to the row of `multiannotator_labels`.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bafbf8",
   "metadata": {},
   "source": [
    "Before training our machine learning model, we must first obtain the consensus labels from the annotators that labeled the data. The simplest way to obtain an initial set of consensus labels is to select it using majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a953e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_labels = get_majority_vote_label(multiannotator_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c114734",
   "metadata": {},
   "source": [
    "## 3. Train model and use cleanlab to get better consensus labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31453ef",
   "metadata": {},
   "source": [
    "Next, we will train our model on the consensus labels obtained using majority vote to compute out-of-sample predicted probabilities. We will then use these `pred_probs` to generate more informed `consensus_labels` using Cleanlab's [CROWDLAB](link to something) algorithm. We them use these `consensus_labels` to train a better model that generates more accurate `pred_probs`. This process itterates until the `consensus_labels` have no more improvement.\n",
    "\n",
    "`train_model()` trains a `resnet18` image model using cross validation to get out-of-sample predicted probabilities on the whole dataset. The function can be replaced with a custom training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"resnet18\" # You can also try with \"swin_base_patch4_window7_224\"\n",
    "\n",
    "# Load model arguments\n",
    "train_args = {\n",
    "    \"num_cv_folds\": 5, \n",
    "    \"verbose\": 1, \n",
    "    \"epochs\": 1, \n",
    "    \"holdout_frac\": 0.2, \n",
    "    \"time_limit\": 21600, \n",
    "    \"random_state\": 123\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and retrain model on better consensus labels, save results\n",
    "indices_changed = set()\n",
    "seen_consensus_labels = list()\n",
    "model_results = {}\n",
    "itter = 0\n",
    "\n",
    "while tuple(consensus_labels) not in seen_consensus_labels:\n",
    "    seen_consensus_labels.append(tuple(consensus_labels)) # add curent consensus labels into the set\n",
    "    model_results['itter'] = itter\n",
    "    model_xval_results_folder = f'{experiment_path}/xval_results_itter{itter}' # + [model_type]\n",
    "\n",
    "    # Zip consensus labels with their corresponding image_paths\n",
    "    consensus_data = pd.DataFrame(zip(image_paths,consensus_labels), columns=[\"image\", \"label\"])\n",
    "    \n",
    "    # Train model\n",
    "    train_model(model_type, consensus_data, model_xval_results_folder, **train_args)\n",
    "    pred_probs, labels, images = sum_xval_folds(model_type, model_xval_results_folder, **train_args)\n",
    "    \n",
    "    # Get improved consensus labels with label quality multiannotator using model pred probs\n",
    "    label_quality_multiannotator = get_label_quality_multiannotator(multiannotator_labels, pred_probs, verbose=False)\n",
    "    consensus_labels = label_quality_multiannotator[\"label_quality\"][\"consensus_label\"].tolist()\n",
    "    \n",
    "    unique_indices = len(indices_changed)\n",
    "    indices_changed.update(list(np.where(consensus_labels != np.array(seen_consensus_labels[-1]))[0]))\n",
    "    unique_indices = len(indices_changed) - unique_indices\n",
    "    label_changes_from_prior = np.sum(consensus_labels != np.array(seen_consensus_labels[-1]))\n",
    "    \n",
    "    print(\"Num changes in consensus labels from previous itter: \", np.sum(consensus_labels != np.array(seen_consensus_labels[-1])))\n",
    "    print(\"Num unique indices changed: \", unique_indices)\n",
    "    # End to delete\n",
    "    \n",
    "    results = {\n",
    "        \"pred_probs\": pred_probs,\n",
    "        \"consensus_labels_in\": labels, # consensus labels used to train the model\n",
    "        \"images\": images, \n",
    "        \"consensus_labels_out\": consensus_labels, # new consensus labels generated from pred_probs\n",
    "        \"label_changes_from_prior\": label_changes_from_prior, # num changes in consensus labels from previous itterations\n",
    "        \"unique_indices_added\": unique_indices, # number of unique labels indices changed\n",
    "    }\n",
    "    \n",
    "    model_results[itter] = results\n",
    "    itter+=1\n",
    "    \n",
    "    if unique_indices == 0: # no more label improvement\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa26e05",
   "metadata": {},
   "source": [
    "## 4. Measure consensus label accuracy and model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e60410",
   "metadata": {},
   "source": [
    "Since our annotators annotated a dataset `cifar10` to which there exist highly accurate `true_labels`, we can measure and report the accuracy of our methods against the ground truth. \n",
    "\n",
    "In a true multiannotator setting this would not be possible since ground truth labels will not exist. Instead we have [benchmarked](link paper or blogpost maybe) this method to ensure this method produces more accurate `consensus_labels` than common inustry methods for getting a consensus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3758a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and load ground truth labels (once again this is only for metrics, normally not have this information)\n",
    "!cd $experiment_path && wget -nc 'https://cleanlab-public.s3.amazonaws.com/Multiannotator/cifar-10h/cifar-10h-worst25-coin20/c10h_test_labels.npy'\n",
    "\n",
    "true_labels = np.load(f'{experiment_path}/c10h_test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06915543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CROWDLAB performance against the ground truth for each epoch\n",
    "for i in range(itter):\n",
    "    pred_probs = model_results[i]['pred_probs']\n",
    "    consensus_labels_in = model_results[i]['consensus_labels_in']\n",
    "    consensus_labels_out = model_results[i]['consensus_labels_out']\n",
    "\n",
    "    acc_model_gtruth = (pred_probs.argmax(axis=1) == true_labels).mean()\n",
    "    acc_consensus_gtruth = (consensus_labels_in == true_labels).mean()\n",
    "\n",
    "    results = {\n",
    "        \"consensus_gtruth_accuracy\": acc_consensus_gtruth, # consensus labels accuracy \n",
    "        \"model_gtruth_accuracy\": acc_model_gtruth,         # model label accuracy\n",
    "    }\n",
    "    \n",
    "    model_results[i].update(results)\n",
    "\n",
    "# Calculate accuracy of final CROWDLAB generated consensus label\n",
    "final_consensus_labels = model_results[itter-1][\"consensus_labels_out\"]\n",
    "final_consensus_accuracy = (final_consensus_labels == true_labels).mean()\n",
    "\n",
    "print(\"Final consensus label accuracy (vs ground truth): \", final_consensus_accuracy)\n",
    "print(\"Final model predictions accuracy (vs ground truth): \", model_results[itter-1][\"model_gtruth_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb6b74",
   "metadata": {},
   "source": [
    "Finally lets plot the results over multiple epochs and observe the improvement. Here epoch -1 corresponds to the initial `consensus_labels` generated using majority vote without the help of a model. Generating consensus with majority vote is the most common practice and therefore makes a great baseline to compare CROWDLAB against.\n",
    "\n",
    "As you can see, the accuracy of the `consensus_labels` improves over several itterations of model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_gtruth_accuracy = [model_results[i][\"consensus_gtruth_accuracy\"] \n",
    "                             for i in range(itter)] + [final_consensus_accuracy]\n",
    "\n",
    "model_gtruth_accuracy = [model_results[i][\"model_gtruth_accuracy\"] for i in range(itter)]\n",
    "\n",
    "\n",
    "# plot prc\n",
    "plt.rcParams[\"figure.figsize\"] = (17,6)\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(-1, itter),consensus_gtruth_accuracy)\n",
    "plt.xlabel(\"Epochs\", fontsize=14)\n",
    "plt.ylabel(\"Consensus Label Accuracy\", fontsize=14)\n",
    "plt.title(\"Consensus Label vs Ground Truth Accuracy\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, itter), model_gtruth_accuracy)\n",
    "plt.xlabel(\"Epochs\", fontsize=14)\n",
    "plt.ylabel(\"Model Prediction Accuracy\", fontsize=14)\n",
    "plt.title(\"Model Predictions vs Ground Truth Accuracy\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac8b61",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "if consensus_gtruth_accuracy[0] >= consensus_gtruth_accuracy[-1]:  # check cleanlab has improved prediction accuracy\n",
    "    raise Exception(\"Cleanlab failed to improve baseline consensus label accuracy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
